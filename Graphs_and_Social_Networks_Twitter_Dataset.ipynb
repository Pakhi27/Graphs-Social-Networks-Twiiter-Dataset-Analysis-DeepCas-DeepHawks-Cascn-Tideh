{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CSUXNVN0PyTv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import networkx as nx\n",
        "import tensorflow as tf\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the datasets\n",
        "index_df = pd.read_csv('/content/index.csv')\n",
        "data_df = pd.read_csv('/content/data (1).csv')\n",
        "\n",
        "# Display the first few rows of the index and data files\n",
        "print(\"Index.csv:\")\n",
        "print(index_df.head())\n",
        "\n",
        "print(\"\\nData.csv:\")\n",
        "print(data_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHD787YnTDwt",
        "outputId": "267d8475-38c9-4ffa-a134-bcafb830f831"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index.csv:\n",
            "       tweet_id  post_time_day  start_ind  end_ind\n",
            "0  1.224350e+17       0.926644          1      175\n",
            "1  1.224500e+17       0.968160        176      369\n",
            "2  1.224500e+17       0.969560        370      703\n",
            "3  1.224430e+17       0.949734        704      827\n",
            "4  1.224570e+17       0.987373        828      941\n",
            "\n",
            "Data.csv:\n",
            "   relative_time_second  number_of_followers\n",
            "0                   0.0                   33\n",
            "1               84833.0                46828\n",
            "2               84878.0                  208\n",
            "3               84883.0                   37\n",
            "4               84900.0                  137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Cascades for Each Tweet"
      ],
      "metadata": {
        "id": "u2VWBmUqVGt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract cascades for each tweet using the index.csv\n",
        "def extract_cascade(tweet_id):\n",
        "    row = index_df[index_df['tweet_id'] == tweet_id]\n",
        "    start_idx = row['start_ind'].values[0]\n",
        "    end_idx = row['end_ind'].values[0]\n",
        "    cascade_data = data_df.iloc[start_idx:end_idx+1]\n",
        "    return cascade_data\n",
        "\n",
        "# Extract all cascades for each tweet\n",
        "cascades = []\n",
        "for tweet_id in index_df['tweet_id'].unique():\n",
        "    cascades.append(extract_cascade(tweet_id))\n",
        "\n",
        "# Display first cascade as an example\n",
        "print(\"First cascade example:\")\n",
        "print(cascades[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guPfvnSXTThF",
        "outputId": "415beb6b-f251-47a3-e08a-cf3bb341d771"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First cascade example:\n",
            "     relative_time_second  number_of_followers\n",
            "1                 84833.0                46828\n",
            "2                 84878.0                  208\n",
            "3                 84883.0                   37\n",
            "4                 84900.0                  137\n",
            "5                 84904.0                  254\n",
            "..                    ...                  ...\n",
            "171              136634.0                   18\n",
            "172              166593.0                   18\n",
            "173              299689.0                   99\n",
            "174              424201.0                  148\n",
            "175                   0.0                40627\n",
            "\n",
            "[175 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "RQO5WZgEVJJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Prepare data for training by extracting followers and time\n",
        "def prepare_data(cascades):\n",
        "    X = []\n",
        "    y = []\n",
        "    for cascade in cascades:\n",
        "        if len(cascade) > 0:  # Check if the cascade is not empty\n",
        "            followers = cascade['number_of_followers'].values\n",
        "            times = cascade['relative_time_second'].values\n",
        "            X.append(np.stack((followers, times), axis=1))  # Combine followers and times as features\n",
        "            y.append(len(cascade))  # Predict the size of the cascade\n",
        "    return X, y\n",
        "\n",
        "# Assuming 'cascades' is your dataset, replace this with your actual data\n",
        "X, y = prepare_data(cascades)\n",
        "\n",
        "# Normalize the time and follower data\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = []\n",
        "for cascade in X:\n",
        "    if len(cascade) > 0:  # Ensure the cascade is not empty before applying the scaler\n",
        "        X_normalized.append(scaler.fit_transform(cascade))\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "X_padded = pad_sequences(X_normalized, padding='post', dtype='float32')\n",
        "\n",
        "# Display prepared features and targets\n",
        "print(f\"Feature shape: {X_padded[0].shape}\")\n",
        "print(f\"Target (cascade sizes): {y[:5]}\")\n"
      ],
      "metadata": {
        "id": "DamQR4K_Tm3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c911a65-d7b0-473f-a22b-ac66ca16598f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature shape: (4409, 2)\n",
            "Target (cascade sizes): [175, 194, 124, 114, 88]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ensure X and y are numpy arrays\n",
        "# Ensure X and y are numpy arrays, then pad sequences in X\n",
        "X = pad_sequences(X, padding='post')  # Pads sequences with zeros at the end\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "nFTRiauuUwSL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split"
      ],
      "metadata": {
        "id": "O5_Ni6J2VSDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Ensure X and y are numpy arrays\n",
        "# X = np.array(X)\n",
        "# y = np.array(y)\n",
        "# # Display shapes of train/test sets\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "YGt5PGceVUmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ee6d39-9be5-4d5c-836c-b4c6519ab30f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (2116, 4409, 2)\n",
            "Test set size: (529, 4409, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implementing DeepCas (RNN-Based Model)"
      ],
      "metadata": {
        "id": "rp6Xc5_TVXGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DeepCas model (LSTM-based RNN)\n",
        "def build_deepcas_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Build and train DeepCas model\n",
        "deepcas_model = build_deepcas_model((X_train.shape[1], 2))\n",
        "history = deepcas_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "deepcas_loss = deepcas_model.evaluate(X_test, y_test)\n",
        "print(f'DeepCas Model Loss: {deepcas_loss}')\n"
      ],
      "metadata": {
        "id": "z5XAwS5TVXx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20619d0-8edd-4dd1-f836-02d47e947f01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 261ms/step - loss: 97703.9609 - val_loss: 148023.4688\n",
            "Epoch 2/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 262ms/step - loss: 88914.4453 - val_loss: 146198.4844\n",
            "Epoch 3/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 253ms/step - loss: 77832.4141 - val_loss: 144590.6406\n",
            "Epoch 4/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 267ms/step - loss: 87428.0781 - val_loss: 143003.7656\n",
            "Epoch 5/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 252ms/step - loss: 85607.0547 - val_loss: 141513.7812\n",
            "Epoch 6/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 272ms/step - loss: 83608.5859 - val_loss: 140059.0312\n",
            "Epoch 7/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 261ms/step - loss: 91342.9688 - val_loss: 138669.9844\n",
            "Epoch 8/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 258ms/step - loss: 70611.3047 - val_loss: 137372.2812\n",
            "Epoch 9/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 252ms/step - loss: 75670.8672 - val_loss: 136111.5781\n",
            "Epoch 10/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 255ms/step - loss: 82365.7188 - val_loss: 134891.9219\n",
            "Epoch 11/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 245ms/step - loss: 74389.5859 - val_loss: 133731.0625\n",
            "Epoch 12/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - loss: 78137.9688 - val_loss: 132609.1250\n",
            "Epoch 13/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 245ms/step - loss: 75141.6250 - val_loss: 131529.8594\n",
            "Epoch 14/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 258ms/step - loss: 67310.2422 - val_loss: 130492.3984\n",
            "Epoch 15/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 247ms/step - loss: 63783.6094 - val_loss: 129498.7812\n",
            "Epoch 16/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 257ms/step - loss: 78874.3750 - val_loss: 128513.5156\n",
            "Epoch 17/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 244ms/step - loss: 68356.1484 - val_loss: 127597.1094\n",
            "Epoch 18/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - loss: 64481.7500 - val_loss: 126734.0625\n",
            "Epoch 19/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 246ms/step - loss: 61309.0469 - val_loss: 125853.2969\n",
            "Epoch 20/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - loss: 60298.7734 - val_loss: 125043.5781\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 134361.4844\n",
            "DeepCas Model Loss: 125043.578125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing DeepHawkes"
      ],
      "metadata": {
        "id": "_Sy_9GXkVZSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the DeepHawkes model (Hawkes process inspired)\n",
        "def build_deephawkes_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Build and train DeepHawkes model\n",
        "deephawkes_model = build_deephawkes_model((X_train.shape[1], 2))\n",
        "history = deephawkes_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "deephawkes_loss = deephawkes_model.evaluate(X_test, y_test)\n",
        "print(f'DeepHawkes Model Loss: {deephawkes_loss}')\n"
      ],
      "metadata": {
        "id": "i1DZ6WKEVdFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f46a53-73d6-4b55-b769-dc1e6a229be7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 269ms/step - loss: 88475.6250 - val_loss: 148499.9062\n",
            "Epoch 2/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 257ms/step - loss: 79824.5703 - val_loss: 146721.6875\n",
            "Epoch 3/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 243ms/step - loss: 87376.6875 - val_loss: 145124.6719\n",
            "Epoch 4/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 258ms/step - loss: 87632.0859 - val_loss: 143618.1562\n",
            "Epoch 5/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 245ms/step - loss: 94885.5234 - val_loss: 142200.9375\n",
            "Epoch 6/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 259ms/step - loss: 77490.1641 - val_loss: 140842.9062\n",
            "Epoch 7/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 247ms/step - loss: 89658.1797 - val_loss: 139516.5781\n",
            "Epoch 8/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 244ms/step - loss: 83195.7109 - val_loss: 138254.0000\n",
            "Epoch 9/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 253ms/step - loss: 85258.8359 - val_loss: 137017.0312\n",
            "Epoch 10/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 245ms/step - loss: 89136.4062 - val_loss: 135862.0781\n",
            "Epoch 11/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 246ms/step - loss: 65234.4453 - val_loss: 134717.5469\n",
            "Epoch 12/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 245ms/step - loss: 75374.9141 - val_loss: 133598.0000\n",
            "Epoch 13/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 245ms/step - loss: 74154.0625 - val_loss: 132529.1562\n",
            "Epoch 14/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 258ms/step - loss: 69425.6641 - val_loss: 131510.6719\n",
            "Epoch 15/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 247ms/step - loss: 68642.0859 - val_loss: 130513.3750\n",
            "Epoch 16/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 260ms/step - loss: 67171.7969 - val_loss: 129554.8906\n",
            "Epoch 17/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 264ms/step - loss: 78098.1172 - val_loss: 128653.9453\n",
            "Epoch 18/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 245ms/step - loss: 84173.9766 - val_loss: 127753.6797\n",
            "Epoch 19/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 254ms/step - loss: 70867.7344 - val_loss: 126889.0312\n",
            "Epoch 20/20\n",
            "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 245ms/step - loss: 70595.8672 - val_loss: 126051.6250\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - loss: 135393.2500\n",
            "DeepHawkes Model Loss: 126051.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing CasCN (Graph-Based Model)"
      ],
      "metadata": {
        "id": "7CzWFmRAVeZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example cascade data (using multiple cascades to match the number of samples in y_train and y_test)\n",
        "cascades = [pd.DataFrame({'number_of_followers': [100, 200], 'relative_time_second': [1, 2]}) for _ in range(len(y_train))]\n",
        "\n",
        "# Graph Construction (example function)\n",
        "def create_graph(cascade):\n",
        "    G = nx.DiGraph()\n",
        "    for i, row in cascade.iterrows():\n",
        "        G.add_node(i, followers=row['number_of_followers'], time=row['relative_time_second'])\n",
        "    return G\n",
        "\n",
        "# Create GCN input based on the number of nodes in each cascade (or any other feature)\n",
        "gcn_input = np.array([len(cascade) for cascade in cascades])\n",
        "\n",
        "# Ensure gcn_input matches y_train size\n",
        "gcn_input = gcn_input.reshape(-1, 1)  # Reshape to have a single feature per sample\n",
        "\n",
        "# Build and train CasCN model\n",
        "cascn_model = build_cas_gcn_model(gcn_input.shape[1])\n",
        "history = cascn_model.fit(gcn_input, y_train, epochs=20, batch_size=32, validation_data=(gcn_input, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "cascn_loss = cascn_model.evaluate(gcn_input, y_test)\n",
        "print(f'CasCN Model Loss: {cascn_loss}')\n"
      ],
      "metadata": {
        "id": "TugZXq55Vgzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89501c46-7976-4654-fd7e-35000bf3dedd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.3858 - val_loss: 0.0844\n",
            "Epoch 2/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0866 - val_loss: 0.0850\n",
            "Epoch 3/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0834 - val_loss: 0.0846\n",
            "Epoch 4/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0858 - val_loss: 0.0846\n",
            "Epoch 5/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0861 - val_loss: 0.0848\n",
            "Epoch 6/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0834 - val_loss: 0.0849\n",
            "Epoch 7/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0882 - val_loss: 0.0848\n",
            "Epoch 8/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0853 - val_loss: 0.0846\n",
            "Epoch 9/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0858 - val_loss: 0.0849\n",
            "Epoch 10/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0878 - val_loss: 0.0859\n",
            "Epoch 11/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0871 - val_loss: 0.0848\n",
            "Epoch 12/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0863 - val_loss: 0.0844\n",
            "Epoch 13/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0859 - val_loss: 0.0858\n",
            "Epoch 14/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0845 - val_loss: 0.0845\n",
            "Epoch 15/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0854 - val_loss: 0.0845\n",
            "Epoch 16/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0852 - val_loss: 0.0846\n",
            "Epoch 17/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0852 - val_loss: 0.0872\n",
            "Epoch 18/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0866 - val_loss: 0.0853\n",
            "Epoch 19/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0900 - val_loss: 0.0844\n",
            "Epoch 20/20\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0857 - val_loss: 0.0848\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0865\n",
            "CasCN Model Loss: 0.08478637039661407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing TiDeH (Time-Dependent Hawkes Process)"
      ],
      "metadata": {
        "id": "XOaIJKBPVik2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have time-series data for each cascade, create X_train and X_test with the shape (samples, timesteps, features)\n",
        "# Here, I'm assuming each cascade has multiple timesteps and two features (relative time and number of followers).\n",
        "# You may need to adjust this based on your actual data.\n",
        "\n",
        "# Example: Reshaping X_train to 3D format\n",
        "# Replace this with actual reshaping logic based on your data\n",
        "X_train = np.random.rand(2169, 10, 2)  # Example shape (2169 samples, 10 timesteps, 2 features)\n",
        "X_test = np.random.rand(2169, 10, 2)   # Example test data with the same shape as X_train\n",
        "\n",
        "# Define the TiDeH model (RNN for temporal dependencies)\n",
        "def build_tideh_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Build and train TiDeH model\n",
        "tideh_model = build_tideh_model((X_train.shape[1], X_train.shape[2]))\n",
        "history = tideh_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "tideh_loss = tideh_model.evaluate(X_test, y_test)\n",
        "print(f'TiDeH Model Loss: {tideh_loss}')\n"
      ],
      "metadata": {
        "id": "zjSBnkPzVlEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3bf11d-cd21-47bb-a9f2-82eaf6aeedf4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 0.1708 - val_loss: 0.0870\n",
            "Epoch 2/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0888 - val_loss: 0.0864\n",
            "Epoch 3/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0848 - val_loss: 0.0924\n",
            "Epoch 4/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0860 - val_loss: 0.0902\n",
            "Epoch 5/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0884 - val_loss: 0.0855\n",
            "Epoch 6/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0866 - val_loss: 0.0874\n",
            "Epoch 7/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0896 - val_loss: 0.0934\n",
            "Epoch 8/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0928 - val_loss: 0.0844\n",
            "Epoch 9/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0838 - val_loss: 0.0850\n",
            "Epoch 10/10\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0882 - val_loss: 0.0852\n",
            "\u001b[1m68/68\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0872\n",
            "TiDeH Model Loss: 0.08515477925539017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing Model Performances"
      ],
      "metadata": {
        "id": "R988_RoXVn1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare performance of all models\n",
        "print(f\"DeepCas Loss: {deepcas_loss}, DeepHawkes Loss: {deephawkes_loss}, CasCN Loss: {cascn_loss}, TiDeH Loss: {tideh_loss}\")\n"
      ],
      "metadata": {
        "id": "ge0RzDYNVoyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef7df4f-b9cf-4fc2-8c97-5d0128cbdaa4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepCas Loss: 125043.578125, DeepHawkes Loss: 126051.625, CasCN Loss: 0.08478637039661407, TiDeH Loss: 0.08515477925539017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z-FYPhgo6vNM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}